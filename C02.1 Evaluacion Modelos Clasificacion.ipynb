{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"logo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conceptos de Clasificación binaria**\n",
    "\n",
    "En clasificación binaria, tenemos el concepto de casos negativos (clase 0, en el caso del dataset de cancer de mama serian los casos donde el cancer es benigno) y casos positivos (clase 1, en el caso del dataset de cancer de mama serían los casos donde el cancer es maligno). Positivo y negativo no significa que el resultado sea bueno o malo, simplemente que la detección de un cancer maligno se active o no.\n",
    "\n",
    "- Casos positivos: Casos donde la clase es 1 (cánceres malignos)\n",
    "- Casos negativos: Casos donde la clase es 0 (cánceres benignos)\n",
    "\n",
    "Esto nos lleva a computar 4 tipos de observaciones, explicados como ejemplos del dataset del cancer de mama.\n",
    "\n",
    "- Verdaderos Positivos(True positives), serían las imágenes con un cancer maligno que se detectan como cancer maligno.\n",
    "- Falsos Positivos (False positives), serían los cánceres benignos que se detectan como un cancer maligno.\n",
    "- Verdaderos Negativos(True Negatives), serían los canceres benignos que se clasifican como cánceres benignos.\n",
    "- Falsos Negativos(False Negatives), serían los canceres malignos que se clasifican como cánceres benignos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ml13.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratios de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exactitud (Accuracy)**\n",
    "\n",
    "La exactitud es una medida general de como se comporta el modelo, mide simplemente el porcentaje de casos que se han clasificado correctamente.\n",
    "\n",
    "$$Exactitud=\\frac{Número~de~observaciones~correctamente~clasificadas}{Número~de~observaciones~totales}= \\frac{VP+VN}{VP+VN+FP+FN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(objetivos_reales, predicciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precisión (Precission)**\n",
    "\n",
    "La precisión indica la habilidad del modelo para clasificar como positivos los casos que son positivos.\n",
    "\n",
    "$$Precisión=\\frac{Número~de~observaciones~positivas~correctamente~clasificadas}{Número~de~observaciones~clasificadas~como~positivas}= \\frac{VP}{VP+FP}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(clases_reales, predicciones):\n",
    "    vp = VP(clases_reales, predicciones)\n",
    "    fp = FP(clases_reales, predicciones)\n",
    "    return vp / (vp+fp)\n",
    "\n",
    "precision(objetivos_reales, predicciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exhaustividad o sensibilidad(Recall o True Positive Rate)**\n",
    "\n",
    "La sensibilidad nos da una medida de la habilidad del modelo para encontrar todos los casos positivos. La sensibilidad se mide en función de una clase.\n",
    "\n",
    "$$Sensibilidad=\\frac{Número~de~observaciones~positivas~clasificadas~como~positivas}{Número~de~observaciones~positivas~totales}= \\frac{VP}{VP+FN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.recall_score(objetivos_reales, predicciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Puntuación F1 (F1 score)**\n",
    "\n",
    "La puntuación F1 es una media ponderada entre la sensibilidad (que intenta obtener cuantos mas verdaderos positivos independientemente de los falsos positivos) y la precisión (que intenta obtener solo verdaderos positivos que sean casos claros para limitar los falsos positivos).\n",
    "\n",
    "La puntuación F1 se define como la media harmónica de la precisión y la sensibilidad:\n",
    "\n",
    "$$F1=2*\\frac{1}{\\frac{1}{precisión}+\\frac{1}{sensibilidad}}=2*\\frac{precisión*sensibilidad}{precisión+sensibilidad}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1_score(objetivos_reales, predicciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ratio de Falsos Positivos (Ratio de Falsa Alarma o FPR)**\n",
    "\n",
    "El ratio de falsos positivos nos da una medida de las probabilidades de nuestro modelo de asignar una clase positiva a un caso negativo.\n",
    "\n",
    "Se define como:\n",
    "\n",
    "$$FPR=\\frac{Número~de~observaciones~negativas~clasificadas~como~positivas}{Número~de~observaciones~negativas}= \\frac{FP}{FP+TN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fpr(clases_reales, predicciones):\n",
    "    return (FP(clases_reales, predicciones) / (\n",
    "             FP(clases_reales, predicciones) + VN(clases_reales, predicciones)\n",
    "             )\n",
    "           )\n",
    "fpr(objetivos_reales, predicciones)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "state": {
    "faecd23e356c4a88b55f22e7d579b0f3": {
     "views": [
      {
       "cell_index": 34
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
